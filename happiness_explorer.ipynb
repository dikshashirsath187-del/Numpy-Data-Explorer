{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Happiness Report Data Explorer\n",
    "## Interactive Analysis with NumPy\n",
    "\n",
    "This notebook provides an interactive exploration of the World Happiness Report 2020 dataset using NumPy for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "def load_happiness_data(csv_file: str):\n",
    "    \"\"\"Load World Happiness Report data into NumPy arrays.\"\"\"\n",
    "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        headers = next(reader)\n",
    "        \n",
    "        # Create column mapping\n",
    "        column_map = {header: i for i, header in enumerate(headers)}\n",
    "        \n",
    "        # Read data\n",
    "        country_names = []\n",
    "        regional_indicators = []\n",
    "        rows = []\n",
    "        \n",
    "        for row in reader:\n",
    "            if row and len(row) > 2:\n",
    "                country_names.append(row[0])\n",
    "                regional_indicators.append(row[1])\n",
    "                \n",
    "                # Extract numeric values\n",
    "                numeric_row = []\n",
    "                for val in row[2:]:\n",
    "                    try:\n",
    "                        numeric_row.append(float(val))\n",
    "                    except ValueError:\n",
    "                        numeric_row.append(np.nan)\n",
    "                rows.append(numeric_row)\n",
    "        \n",
    "        data = np.array(rows)\n",
    "        \n",
    "    return data, headers, country_names, regional_indicators, column_map\n",
    "\n",
    "# Load the data\n",
    "data, headers, countries, regions, col_map = load_happiness_data('WHR20_DataForFigure2.1.csv')\n",
    "\n",
    "print(f\"Loaded {len(countries)} countries with {data.shape[1]} features\")\n",
    "print(f\"\\nFirst 5 countries: {countries[:5]}\")\n",
    "print(f\"\\nData shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all column names\n",
    "print(\"Available columns:\")\n",
    "print(\"=\" * 60)\n",
    "for i, header in enumerate(headers):\n",
    "    print(f\"{i:2d}. {header}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique regions\n",
    "unique_regions = list(set(regions))\n",
    "print(f\"\\nNumber of unique regions: {len(unique_regions)}\")\n",
    "print(\"\\nRegions:\")\n",
    "print(\"=\" * 60)\n",
    "for i, region in enumerate(sorted(unique_regions), 1):\n",
    "    count = regions.count(region)\n",
    "    print(f\"{i:2d}. {region:40s} ({count} countries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get column index\n",
    "def get_col_idx(column_name: str) -> int:\n",
    "    \"\"\"Get column index adjusted for numeric data (skip first 2 columns).\"\"\"\n",
    "    return col_map[column_name] - 2\n",
    "\n",
    "# Analyze Ladder Score (Happiness Score)\n",
    "ladder_idx = get_col_idx('Ladder score')\n",
    "ladder_scores = data[:, ladder_idx]\n",
    "\n",
    "print(\"HAPPINESS SCORE (Ladder Score) STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean:       {np.nanmean(ladder_scores):.4f}\")\n",
    "print(f\"Median:     {np.nanmedian(ladder_scores):.4f}\")\n",
    "print(f\"Std Dev:    {np.nanstd(ladder_scores):.4f}\")\n",
    "print(f\"Min:        {np.nanmin(ladder_scores):.4f}\")\n",
    "print(f\"Max:        {np.nanmax(ladder_scores):.4f}\")\n",
    "print(f\"Range:      {np.nanmax(ladder_scores) - np.nanmin(ladder_scores):.4f}\")\n",
    "\n",
    "# Percentiles\n",
    "print(\"\\nPercentiles:\")\n",
    "for p in [25, 50, 75, 90, 95]:\n",
    "    val = np.nanpercentile(ladder_scores, p)\n",
    "    print(f\"  {p:2d}th percentile: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Country Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 Happiest Countries\n",
    "sorted_indices = np.argsort(ladder_scores)[::-1]\n",
    "\n",
    "print(\"TOP 15 HAPPIEST COUNTRIES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Rank':<6} {'Country':<30} {'Region':<25} {'Score':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for rank, idx in enumerate(sorted_indices[:15], 1):\n",
    "    country = countries[idx]\n",
    "    region = regions[idx]\n",
    "    score = ladder_scores[idx]\n",
    "    print(f\"{rank:<6} {country:<30} {region[:23]:<25} {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom 15 Countries\n",
    "print(\"BOTTOM 15 COUNTRIES BY HAPPINESS SCORE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Rank':<6} {'Country':<30} {'Region':<25} {'Score':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for rank, idx in enumerate(sorted_indices[-15:][::-1], 1):\n",
    "    country = countries[idx]\n",
    "    region = regions[idx]\n",
    "    score = ladder_scores[idx]\n",
    "    print(f\"{rank:<6} {country:<30} {region[:23]:<25} {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare happiness scores across regions\n",
    "def analyze_by_region(column_name: str):\n",
    "    \"\"\"Analyze a metric by region.\"\"\"\n",
    "    col_idx = get_col_idx(column_name)\n",
    "    values = data[:, col_idx]\n",
    "    \n",
    "    results = []\n",
    "    for region in sorted(set(regions)):\n",
    "        region_mask = np.array([r == region for r in regions])\n",
    "        region_values = values[region_mask]\n",
    "        clean_values = region_values[~np.isnan(region_values)]\n",
    "        \n",
    "        if len(clean_values) > 0:\n",
    "            results.append({\n",
    "                'region': region,\n",
    "                'mean': np.mean(clean_values),\n",
    "                'median': np.median(clean_values),\n",
    "                'std': np.std(clean_values),\n",
    "                'count': len(clean_values)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "regional_happiness = analyze_by_region('Ladder score')\n",
    "regional_happiness.sort(key=lambda x: x['mean'], reverse=True)\n",
    "\n",
    "print(\"HAPPINESS SCORE BY REGION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Region':<35} {'Mean':<8} {'Median':<8} {'Std Dev':<8} {'Count':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for stats in regional_happiness:\n",
    "    print(f\"{stats['region'][:33]:<35} {stats['mean']:7.3f}  {stats['median']:7.3f}  \"\n",
    "          f\"{stats['std']:7.3f}  {stats['count']:7d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key factors contributing to happiness\n",
    "factors = [\n",
    "    'Logged GDP per capita',\n",
    "    'Social support',\n",
    "    'Healthy life expectancy',\n",
    "    'Freedom to make life choices',\n",
    "    'Generosity',\n",
    "    'Perceptions of corruption'\n",
    "]\n",
    "\n",
    "print(\"KEY HAPPINESS FACTORS - STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Factor':<35} {'Mean':<10} {'Std Dev':<10} {'Min':<10} {'Max':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for factor in factors:\n",
    "    col_idx = get_col_idx(factor)\n",
    "    values = data[:, col_idx]\n",
    "    clean_values = values[~np.isnan(values)]\n",
    "    \n",
    "    print(f\"{factor[:33]:<35} {np.mean(clean_values):9.4f}  {np.std(clean_values):9.4f}  \"\n",
    "          f\"{np.min(clean_values):9.4f}  {np.max(clean_values):9.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with happiness score\n",
    "ladder_idx = get_col_idx('Ladder score')\n",
    "ladder_data = data[:, ladder_idx]\n",
    "\n",
    "print(\"CORRELATION WITH HAPPINESS SCORE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Factor':<40} {'Correlation':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "correlations = []\n",
    "for factor in factors:\n",
    "    col_idx = get_col_idx(factor)\n",
    "    factor_data = data[:, col_idx]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_mask = ~(np.isnan(ladder_data) | np.isnan(factor_data))\n",
    "    \n",
    "    if np.sum(valid_mask) > 0:\n",
    "        corr = np.corrcoef(ladder_data[valid_mask], factor_data[valid_mask])[0, 1]\n",
    "        correlations.append((factor, corr))\n",
    "\n",
    "# Sort by correlation strength\n",
    "correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "for factor, corr in correlations:\n",
    "    bar_length = int(abs(corr) * 30)\n",
    "    bar = 'â–ˆ' * bar_length\n",
    "    print(f\"{factor[:38]:<40} {corr:6.3f}  {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Matrix for Key Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "selected_factors = [\n",
    "    'Ladder score',\n",
    "    'Logged GDP per capita',\n",
    "    'Social support',\n",
    "    'Healthy life expectancy',\n",
    "    'Freedom to make life choices'\n",
    "]\n",
    "\n",
    "# Extract data for selected factors\n",
    "factor_indices = [get_col_idx(f) for f in selected_factors]\n",
    "factor_data = data[:, factor_indices]\n",
    "\n",
    "# Remove rows with any NaN\n",
    "valid_rows = ~np.any(np.isnan(factor_data), axis=1)\n",
    "clean_data = factor_data[valid_rows]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = np.corrcoef(clean_data.T)\n",
    "\n",
    "print(\"CORRELATION MATRIX\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Print header\n",
    "print(f\"{'Factor':<30}\", end=\"\")\n",
    "for i in range(len(selected_factors)):\n",
    "    print(f\"  {i+1:5}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Print matrix\n",
    "for i, factor in enumerate(selected_factors):\n",
    "    print(f\"{i+1}. {factor[:27]:<28}\", end=\"\")\n",
    "    for j in range(len(selected_factors)):\n",
    "        print(f\" {corr_matrix[i, j]:6.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nNote: Values close to 1 indicate strong positive correlation\")\n",
    "print(\"      Values close to -1 indicate strong negative correlation\")\n",
    "print(\"      Values close to 0 indicate weak or no correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Country-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific countries\n",
    "def get_country_profile(country_name: str):\n",
    "    \"\"\"Get complete profile for a country.\"\"\"\n",
    "    try:\n",
    "        idx = countries.index(country_name)\n",
    "        \n",
    "        print(f\"\\nCOUNTRY PROFILE: {country_name.upper()}\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Region: {regions[idx]}\")\n",
    "        print()\n",
    "        \n",
    "        # Get rank\n",
    "        ladder_idx = get_col_idx('Ladder score')\n",
    "        country_score = data[idx, ladder_idx]\n",
    "        rank = np.sum(data[:, ladder_idx] > country_score) + 1\n",
    "        percentile = (1 - rank / len(countries)) * 100\n",
    "        \n",
    "        print(f\"Happiness Rank: #{rank} out of {len(countries)} ({percentile:.1f}th percentile)\")\n",
    "        print(f\"Happiness Score: {country_score:.3f}\")\n",
    "        print()\n",
    "        \n",
    "        # Key metrics\n",
    "        print(\"Key Metrics:\")\n",
    "        print(\"-\" * 70)\n",
    "        key_metrics = [\n",
    "            'Logged GDP per capita',\n",
    "            'Social support',\n",
    "            'Healthy life expectancy',\n",
    "            'Freedom to make life choices',\n",
    "            'Generosity',\n",
    "            'Perceptions of corruption'\n",
    "        ]\n",
    "        \n",
    "        for metric in key_metrics:\n",
    "            col_idx = get_col_idx(metric)\n",
    "            value = data[idx, col_idx]\n",
    "            \n",
    "            # Calculate percentile\n",
    "            all_values = data[:, col_idx]\n",
    "            clean_values = all_values[~np.isnan(all_values)]\n",
    "            perc = (np.sum(clean_values < value) / len(clean_values)) * 100\n",
    "            \n",
    "            print(f\"  {metric:<40} {value:8.3f}  ({perc:5.1f}th percentile)\")\n",
    "        \n",
    "    except ValueError:\n",
    "        print(f\"Country '{country_name}' not found in dataset.\")\n",
    "\n",
    "# Analyze multiple countries\n",
    "sample_countries = ['Finland', 'United States', 'India', 'China', 'Brazil']\n",
    "\n",
    "for country in sample_countries:\n",
    "    get_country_profile(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Custom Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find countries similar to a given country\n",
    "def find_similar_countries(country_name: str, n: int = 5):\n",
    "    \"\"\"Find countries with similar happiness profiles.\"\"\"\n",
    "    try:\n",
    "        idx = countries.index(country_name)\n",
    "        country_data = data[idx, :]\n",
    "        \n",
    "        # Calculate Euclidean distance for key factors\n",
    "        key_factors = ['Ladder score', 'Logged GDP per capita', 'Social support', \n",
    "                      'Healthy life expectancy', 'Freedom to make life choices']\n",
    "        key_indices = [get_col_idx(f) for f in key_factors]\n",
    "        \n",
    "        distances = []\n",
    "        for i, other_country in enumerate(countries):\n",
    "            if i != idx:\n",
    "                other_data = data[i, key_indices]\n",
    "                country_subset = country_data[key_indices]\n",
    "                \n",
    "                # Skip if any NaN values\n",
    "                if not (np.any(np.isnan(other_data)) or np.any(np.isnan(country_subset))):\n",
    "                    dist = np.linalg.norm(country_subset - other_data)\n",
    "                    distances.append((i, dist))\n",
    "        \n",
    "        # Sort by distance and get top N\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"\\nCOUNTRIES MOST SIMILAR TO {country_name.upper()}\")\n",
    "        print(\"=\" * 70)\n",
    "        ladder_idx = get_col_idx('Ladder score')\n",
    "        \n",
    "        for i, (country_idx, dist) in enumerate(distances[:n], 1):\n",
    "            similar_country = countries[country_idx]\n",
    "            score = data[country_idx, ladder_idx]\n",
    "            region = regions[country_idx]\n",
    "            print(f\"{i}. {similar_country:<30} Score: {score:.3f}  Region: {region}\")\n",
    "        \n",
    "    except ValueError:\n",
    "        print(f\"Country '{country_name}' not found.\")\n",
    "\n",
    "# Test the function\n",
    "find_similar_countries('India', 7)\n",
    "find_similar_countries('United States', 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find countries that excel in specific areas\n",
    "def find_top_performers(metric: str, region: str = None, n: int = 10):\n",
    "    \"\"\"Find top performing countries in a specific metric.\"\"\"\n",
    "    col_idx = get_col_idx(metric)\n",
    "    values = data[:, col_idx]\n",
    "    \n",
    "    # Filter by region if specified\n",
    "    if region:\n",
    "        mask = np.array([r == region for r in regions])\n",
    "        indices = np.where(mask)[0]\n",
    "    else:\n",
    "        indices = np.arange(len(countries))\n",
    "    \n",
    "    # Get valid values\n",
    "    valid_indices = [i for i in indices if not np.isnan(values[i])]\n",
    "    valid_values = values[valid_indices]\n",
    "    \n",
    "    # Sort and get top N\n",
    "    sorted_idx = np.argsort(valid_values)[::-1][:n]\n",
    "    \n",
    "    region_str = f\" in {region}\" if region else \"\"\n",
    "    print(f\"\\nTOP {n} COUNTRIES BY {metric.upper()}{region_str}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for rank, idx in enumerate(sorted_idx, 1):\n",
    "        country_idx = valid_indices[idx]\n",
    "        country = countries[country_idx]\n",
    "        value = valid_values[idx]\n",
    "        country_region = regions[country_idx]\n",
    "        print(f\"{rank:2d}. {country:<30} {value:8.3f}  ({country_region})\")\n",
    "\n",
    "# Examples\n",
    "find_top_performers('Generosity', n=10)\n",
    "find_top_performers('Social support', region='South Asia', n=5)\n",
    "find_top_performers('Freedom to make life choices', n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total countries: {len(countries)}\")\n",
    "print(f\"Total features: {data.shape[1]}\")\n",
    "print()\n",
    "\n",
    "print(\"Missing Values by Column:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, header in enumerate(headers[2:]):\n",
    "    col_data = data[:, i]\n",
    "    missing_count = np.sum(np.isnan(col_data))\n",
    "    missing_pct = (missing_count / len(col_data)) * 100\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        print(f\"{header:<45} {missing_count:3d} ({missing_pct:5.1f}%)\")\n",
    "\n",
    "# Check for outliers\n",
    "print(\"\\n\\nPotential Outliers (Z-score > 2.5):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "key_columns = ['Ladder score', 'Logged GDP per capita', 'Social support']\n",
    "\n",
    "for col_name in key_columns:\n",
    "    col_idx = get_col_idx(col_name)\n",
    "    values = data[:, col_idx]\n",
    "    \n",
    "    mean = np.nanmean(values)\n",
    "    std = np.nanstd(values)\n",
    "    z_scores = np.abs((values - mean) / std)\n",
    "    \n",
    "    outlier_indices = np.where(z_scores > 2.5)[0]\n",
    "    \n",
    "    if len(outlier_indices) > 0:\n",
    "        print(f\"\\n{col_name}:\")\n",
    "        for idx in outlier_indices:\n",
    "            print(f\"  {countries[idx]:<30} Value: {values[idx]:7.3f}, Z-score: {z_scores[idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"WORLD HAPPINESS REPORT 2020 - KEY INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Global statistics\n",
    "ladder_idx = get_col_idx('Ladder score')\n",
    "ladder_scores = data[:, ladder_idx]\n",
    "\n",
    "print(\"\\n1. GLOBAL HAPPINESS OVERVIEW\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Average happiness score: {np.nanmean(ladder_scores):.3f}\")\n",
    "print(f\"   Happiest country: {countries[np.nanargmax(ladder_scores)]} ({np.nanmax(ladder_scores):.3f})\")\n",
    "print(f\"   Least happy country: {countries[np.nanargmin(ladder_scores)]} ({np.nanmin(ladder_scores):.3f})\")\n",
    "print(f\"   Happiness gap: {np.nanmax(ladder_scores) - np.nanmin(ladder_scores):.3f}\")\n",
    "\n",
    "# Regional insights\n",
    "print(\"\\n2. REGIONAL PATTERNS\")\n",
    "print(\"-\" * 80)\n",
    "regional_stats = analyze_by_region('Ladder score')\n",
    "regional_stats.sort(key=lambda x: x['mean'], reverse=True)\n",
    "print(f\"   Happiest region: {regional_stats[0]['region']} (avg: {regional_stats[0]['mean']:.3f})\")\n",
    "print(f\"   Least happy region: {regional_stats[-1]['region']} (avg: {regional_stats[-1]['mean']:.3f})\")\n",
    "\n",
    "# Factor insights\n",
    "print(\"\\n3. KEY DRIVERS OF HAPPINESS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   Strongest correlations with happiness:\")\n",
    "for factor, corr in correlations[:3]:\n",
    "    print(f\"   - {factor}: r = {corr:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates comprehensive data analysis using NumPy for:\n",
    "- Data loading and preprocessing\n",
    "- Statistical analysis\n",
    "- Regional comparisons\n",
    "- Correlation analysis\n",
    "- Country-specific profiling\n",
    "- Custom queries and insights\n",
    "\n",
    "Feel free to modify the code cells above to explore other aspects of the data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
